## Adversarial Interpretability

The goal of this work is to study whether an adversarially robust model is more intrepretable
in terms of feature visualizations and auralizations.

## Related Work


## Experiment 1

The first experiment is to benchmark adversarial attacks on a normally trained model in order
to understand the limits of how much perturbation is required to generate an adversarial
attack for this task

